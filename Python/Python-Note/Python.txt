Python is a dynamic and strongly typed programming language designed to emphasize usability. Two similar but incompatible versions of Python are in widespread use (2 and 3).


Python will fall under byte code interpreted. . py source code is first compiled to byte code as .pyc. This byte code can be interpreted (official CPython), or JIT compiled (PyPy). Python source code (.py) can be compiled to different byte code also like IronPython (.NET) or Jython (JVM).


Is Python Interpreted or Compiled?

It's worth noting that languages are not interpreted or compiled, but rather language implementations either interpret or compile code. You noted that Ruby is an "interpreted language", but you can compile Ruby à la MacRuby, so it's not always an interpreted language.

Pretty much every Python implementation consists of an interpreter (rather than a compiler). The .pyc files you see are byte code for the Python virtual machine (similar to Java's .class files). They are not the same as the machine code generated by a C compiler for a native machine architecture. Some Python implementations, however, do consist of a just-in-time compiler that will compile Python byte code into native machine code.

(I say "pretty much every" because I don't know of any native machine compilers for Python, but I don't want to claim that none exist anywhere.)


Python Process

The bytecode is not actually interpreted to machine code, unless you are using some exotic implementation such as pypy.

Other than that, you have the description correct. The bytecode is loaded into the Python runtime and interpreted by a virtual machine, which is a piece of code that reads each instruction in the bytecode and executes whatever operation is indicated. You can see this bytecode with the dis module, as follows:

>>> def fib(n): return n if n < 2 else fib(n - 2) + fib(n - 1)
... 
>>> fib(10)
55
>>> import dis
>>> dis.dis(fib)
  1           0 LOAD_FAST                0 (n)
              3 LOAD_CONST               1 (2)
              6 COMPARE_OP               0 (<)
              9 JUMP_IF_FALSE            5 (to 17)
             12 POP_TOP             
             13 LOAD_FAST                0 (n)
             16 RETURN_VALUE        
        >>   17 POP_TOP             
             18 LOAD_GLOBAL              0 (fib)
             21 LOAD_FAST                0 (n)
             24 LOAD_CONST               1 (2)
             27 BINARY_SUBTRACT     
             28 CALL_FUNCTION            1
             31 LOAD_GLOBAL              0 (fib)
             34 LOAD_FAST                0 (n)
             37 LOAD_CONST               2 (1)
             40 BINARY_SUBTRACT     
             41 CALL_FUNCTION            1
             44 BINARY_ADD          
             45 RETURN_VALUE        
>>> 
Detailed explanation
It is quite important to understand that the above code is never executed by your CPU; nor is it ever converted into something that is (at least, not on the official C implementation of Python). The CPU executes the virtual machine code, which performs the work indicated by the bytecode instructions. When the interpreter wants to execute the fib function, it reads the instructions one at a time, and does what they tell it to do. It looks at the first instruction, LOAD_FAST 0, and thus grabs parameter 0 (the n passed to fib) from wherever parameters are held and pushes it onto the interpreter's stack (Python's interpreter is a stack machine). On reading the next instruction, LOAD_CONST 1, it grabs constant number 1 from a collection of constants owned by the function, which happens to be the number 2 in this case, and pushes that onto the stack. You can actually see these constants:

>>> fib.func_code.co_consts
(None, 2, 1)
The next instruction, COMPARE_OP 0, tells the interpreter to pop the two topmost stack elements and perform an inequality comparison between them, pushing the Boolean result back onto the stack. The fourth instruction determines, based on the Boolean value, whether to jump forward five instructions or continue on with the next instruction. All that verbiage explains the if n < 2 part of the conditional expression in fib. It will be a highly instructive exercise for you to tease out the meaning and behaviour of the rest of the fib bytecode. The only one, I'm not sure about is POP_TOP; I'm guessing JUMP_IF_FALSE is defined to leave its Boolean argument on the stack rather than popping it, so it has to be popped explicitly.

Even more instructive is to inspect the raw bytecode for fib thus:

>>> code = fib.func_code.co_code
>>> code
'|\x00\x00d\x01\x00j\x00\x00o\x05\x00\x01|\x00\x00S\x01t\x00\x00|\x00\x00d\x01\x00\x18\x83\x01\x00t\x00\x00|\x00\x00d\x02\x00\x18\x83\x01\x00\x17S'
>>> import opcode
>>> op = code[0]
>>> op
'|'
>>> op = ord(op)
>>> op
124
>>> opcode.opname[op]
'LOAD_FAST'
>>> 
Thus you can see that the first byte of the bytecode is the LOAD_FAST instruction. The next pair of bytes, '\x00\x00' (the number 0 in 16 bits) is the argument to LOAD_FAST, and tells the bytecode interpreter to load parameter 0 onto the stack.


Why doesn't Python need a compiler?

Python has a compiler! You just don't notice it because it runs automatically. You can tell it's there, though: look at the .pyc (or .pyo if you have the optimizer turned on) files that are generated for modules that you import.

Also, it does not compile to the native machine's code. Instead, it compiles to a byte code that is used by a virtual machine. The virtual machine is itself a compiled program. This is very similar to how Java works; so similar, in fact, that there is a Python variant (Jython) that compiles to the Java Virtual Machine's byte code instead! There's also IronPython, which compiles to Microsoft's CLR (used by .NET). (The normal Python byte code compiler is sometimes called CPython to disambiguate it from these alternatives.)

C++ needs to expose its compilation process because the language itself is incomplete; it does not specify everything the linker needs to know to build your program, nor can it specify compile options portably (some compilers let you use #pragma, but that's not standard). So you have to do the rest of the work with makefiles and possibly auto hell (autoconf/automake/libtool). This is really just a holdover from how C did it. And C did it that way because it made the compiler simple, which is one main reason it is so popular (anyone could crank out a simple C compiler in the 80's).

EDIT: Some things that can affect the compiler's or linker's operation but are not specified within C or C++'s syntax:

dependency resolution
external library requirements (including dependency order)
optimizer level
warning settings
language specification version
linker mappings (which section goes where in the final program)
target architecture
Some of these can be detected, but they can't be specified; e.g. I can detect which C++ is in use with __cplusplus, but I can't specify that C++98 is the one used for my code within the code itself; I have to pass it as a flag to the compiler in the Makefile, or make a setting in a dialog.

While you might think that a "dependency resolution" system exists in the compiler, automatically generating dependency records, these records only say which header files a given source file uses. They cannot indicate what additional source code modules are required to link into an executable program, because there is no standard way in C or C++ to indicate that a given header file is the interface definition for another source code module as opposed to just a bunch of lines you want to show up in multiple places so you don't repeat yourself. There are traditions in file naming conventions, but these are not known or enforced by the compiler and linker.

Several of these can be set using #pragma, but this is non-standard, and I was speaking of the standard. All of these things could be specified by a standard, but have not been in the interest of backward compatibility. The prevailing wisdom is that makefiles and IDEs aren't broke, so don't fix them.

Python handles all this in the language. For example, import specifies an explicit module dependency, implies the dependency tree, and modules are not split into header and source files (i.e. interface and implementation).


What are some characteristics of Python that makes it unique as its own language?

You'll have a hard time finding features which are absolutely unique. Most language features in existence have been adopted in more than one language since their inception. Some may be rarer, mostly because they're either new and still in obscurity, or died out for good reason. Nevertheless, even then you'd be better off looking at combinations of features.

That said, several features of Python should make for a relatively unique combination. At least I'm not aware of any languages remotely as popular (and practical) with a mostly overlapping feature set. As noted in comments, Ruby is pretty close, but there are nevertheless numerous differences.

Metaclass-based metaprogramming. Basically, running arbitary code on class creation. Makes for very nice class customization with very little work on the recieving end - e.g. for an ORM, client classes can be written as usual with a few extra lines like attr = SomeDataType() and a ton of code is generated automatically.
You're encouraged to use iterators for everything. This is especially apparent in 3.x, where most list-based alternatives with an iterator-based equivalent have been abolished in favour of the latter. Iterators also serve as nigh-universal interface for collections (both those you actually have in memory and those you only need once and thus create with the features below). Collection-agnostic, space-efficent (O(1) space for intermediate results often follows naturally, very few tasks actually need all items in memory at once), composable data crunching has never been easier.
Generator expressions, related to the above. Many will have heard of list comprehensions (creating a list from another iterable, filtering and mapping in the process, with very convenient syntax). Forget about them, they're syntactic sugar, a special case. Generator expressions are very close in syntactically and ultimately result in the very same sequence of items, but they produce results lazily (and thus take O(1) space unless you explicitly keep the results around).
yield, which mainly make writing iterators (called generators here) far nicer. They're the big brother of the above, supporting all kinds of control flow. C# has something similar, with the same keyword. But yield is also overloaded to support a limited kind of coroutines (Lua for instance has more elaborate support) which has nevertheless been put to good use by clever people working on hard problems. Two examples off the top of my head: Recursive descent parsing with backtracing and no stack limit and asynchronous I/O (with convenient syntax).
Multi-target assignment and iterable unpacking. Assignment on sterioids. Not only you can assign to multiple values at once (even for swapping values and when iterating - for key, value in mapping.items()), you can unpack any iterable of known length (honestly, mostly tuples) into multiple variables. Since 3.x it's even practical for collections of unknown length as you can specify a few variables taking single items and one taking whatever remains: first, *everything_in_between, last = values.
Descriptors, probably the most powerful among the various ways to customize attribute access. There are properties (as in C#, but without special language support), static methods, class methods, etc. all implemented as descriptors. They're first-class objects as well. Just a week ago, I've been faced with repetive and tricky code in properties - so I wrote a small function generating the repetive part and wrapping it up in a propery object.
Purely offside rule (indentaion for delimiting blocks). I put this last intentionally: While it does distinguish Python, it doesn't really stand out in everyday programming once you're used to it (or at least that's my experience).


How would one create an iterable function or class in Python?

Iterator objects in python conform to the iterator protocol, which basically means they provide two methods: __iter__() and  next(). The __iter__ returns the iterator object and is implicitly called at the start of loops. The next() method returns the next value and is implicitly called at each loop increment.  next() raises a StopIteration exception when there are no more value to return, which is implicitly captured by looping constructs to stop iterating.

Here's a simple example of a counter:

class Counter:
    def __init__(self, low, high):
        self.current = low
        self.high = high

    def __iter__(self):
        return self

    def next(self): # Python 3: def __next__(self)
        if self.current > self.high:
            raise StopIteration
        else:
            self.current += 1
            return self.current - 1


for c in Counter(3, 8):
    print c
This will print:

3
4
5
6
7
8
This is easier to write using a generator, as covered in a previous answer:

def counter(low, high):
    current = low
    while current <= high:
        yield current
        current += 1

for c in counter(3, 8):
    print c
The printed output will be the same. Under the hood, the generator object supports the iterator protocol and does something roughly similar to the class Counter.

David Mertz's article, Iterators and Simple Generators [https://www.ibm.com/developerworks/library/l-pycon/], is a pretty good introduction.

Charming Python: Iterators and simple generators [https://www.ibm.com/developerworks/library/l-pycon/]
New constructs in Python 2.2
Python 2.2 introduces a new construct accompanied by a new keyword. The construct is generators; the keyword is yield. Generators make possible several new, powerful, and expressive programming idioms, but are also a little bit hard to get one's mind around at first glance. In this article, David provides a gentle introduction to generators, and also to the related topic of iterators.

Welcome to the world of exotic flow control. With Python 2.2 (now in its third alpha release -- see Resources later in this article), programmers will get some new options for making programs tick that were not available -- or at least not as convenient -- in earlier Python versions.
While what Python 2.2 gives us is not quite as mind-melting as the full continuations and microthreads that are possible in Stackless Python, generators and iterators do something a bit different from traditional functions and classes.
Let's consider iterators first, since they are simpler to understand. Basically, an iterator is an object that has a .next() method. Well, that's not quite true; but it's close. Actually, most iterator contexts want an object that will generate an iterator when the new iter() built-in function is applied to it. To have a user-defined class (that has the requisite .next() method) return an iterator, you need to have an __iter__() method return self. The examples will make this all clear. An iterator's .next() method might decide to raise a StopIteration exception if the iteration has a logical termination.
A generator is a little more complicated and general. But the most typical use of generators will be for defining iterators; so some of the subtlety is not always worth worrying about. A generator is a function that remembers the point in the function body where it last returned. Calling a generator function a second (or nth) time jumps into the middle of the function, with all local variables intact from the last invocation.
In some ways, a generator is like the closures which were discussed in previous installments of this column discussing functional programming (see Resources). Like a closure, a generator "remembers" the state of its data. But a generator goes a bit further than a closure: a generator also "remembers" its position within flow-control constructs (which, in imperative programming, is something more than just data values). Continuations are still more general since they let you jump arbitrarily between execution frames, rather than returning always to the immediate caller's context (as a generator does).
Fortunately, using a generator is much less work than understanding all the conceptual issues of program flow and state. In fact, after very little practice, generators seem as obvious as ordinary functions.
Taking a random walk
Let's consider a fairly simple problem that we can solve in several ways -- both new and old. Suppose we want a stream of positive random numbers less than one that obey a backward-looking constraint. Specifically, we want each successive number to be at least 0.4 more or less than the last one. Moreover, the stream itself is not infinite, but rather ends after a random number of steps. For the examples, we will simply end the stream when a number less than 0.1 is produced. The constraints described are a bit like one might find in a "random walk" algorithm, with the end condition resembling a "statisficing" or "local minimum" result -- but certainly the requirements are simpler than most real-world ones.
In Python 2.1 or earlier, we have a few approaches to solving our problem. One approach is to simply produce and return a list of numbers in the stream. This might look like:
import random
def randomwalk_list():
    last, rand = 1, random.random() # init candidate elements
    nums = []                       # empty list
    while rand > 0.1:                # threshhold terminator
        if abs(last-rand) >= 0.4:    # accept the number
            last = rand
            nums.append(rand)       # add latest candidate to nums
        else:
            print '*',               # display the rejection
        rand = random.random()      # new candidate
    nums.append(rand)               # add the final small element
return nums
Utilizing this function is as simple as:
for num in randomwalk_list():
    print num,
There are a few notable limitations to the above approach. The specific example is exceedingly unlikely to produce huge lists; but just by making the threshhold terminator more stringent, we could create arbitrarily large streams (of random exact size, but of anticipatable order-of-magnitude). At a certain point, memory and performance issues can make this approach undesirable and unnecessary. This same concern got xrange() and xreadlines() added to Python in earlier versions. More significantly, many streams depend on external events, and yet should be processed as each element is available. For example, a stream might listen to a port, or wait for user inputs. Trying to create a complete list out of the stream is simply not an option in these cases.
One trick available in Python 2.1 and earlier is to use a "static" function-local variable to remember things about the last invocation of a function. Obviously, global variables could do the same job, but they cause the familiar problems with pollution of the global namespace, and allow mistakes due to non-locality. You might be surprised here if you are unfamiliar with the trick--Python does not have an "official" static scoping declaration. However, if named parameters are given mutable default values, the parameters can act as persistent memories of previous invocations. Lists, specifically, are handy mutable objects that can conveniently even hold multiple values.
Using a "static" approach, we can write a function like:
import random
def randomwalk_static(last=[1]):# init the "static" var(s)
    rand = random.random()          # init a candidate value
    if last[0] < 0.1:                # threshhold terminator
        return None                  # end-of-stream flag
    while abs(last[0]-rand) < 0.4:   # look for usable candidate
        print '*',                   # display the rejection
        rand = random.random()      # new candidate
    last[0] = rand                  # update the "static" var
return rand
This function is quite memory friendly. All it needs to remember is one previous value, and all it returns is a single number (not a big list of them). And a function similar to this could return successive values that depend (partly or wholly) on external events. On the down side, utilizing this function is somewhat less concise, and considerably less elegant:
num = randomwalk_static()
while num is not None:
    print num,
    num = randomwalk_static()

New ways of walking
"Under the hood", Python 2.2 sequences are all iterators. The familiar Python idiom for elem in lst: now actually asks lst to produce an iterator. The for loop then repeatedly calls the .next() method of this iterator until it encounters a StopIteration exception. Luckily, Python programmers do not need to know what is happening here, since all the familiar built-in types produce their iterators automatically. In fact, now dictionaries have the methods .iterkeys(), .iteritems(), and .itervalues() to produce iterators; the first is what gets used in the new idiom for key in dct:. Likewise, the new idiom for line in file: is supported via an iterator that calls .readline().
But given what is actually happening within the Python interpreter, it becomes obvious to use custom classes that produce their own iterators rather than exclusively use the iterators of built-in types. A custom class that enables both the direct usage of randomwalk_list() and the element-at-a-time parsimony of randomwalk_static is straightforward:

import random
class  randomwalk_iter:
def  __init__(self):
    self.last = 1               # init the prior value
    self.rand = random.random() # init a candidate value
def __iter__(self):
    return self                 # simplest iterator creation
def next(self):
    if self.rand < 0.1:         # threshhold terminator
        raise StopIteration     # end of iteration
    else:                       # look for usable candidate
        while abs(self.last-self.rand) < 0.4:
                print '*',           # display the rejection
                self.rand = random.random() # new candidate
            self.last = self.rand   # update prior value
            return self.rand
Use of this custom iterator looks exactly the same as for a true list generated by a function:
for num in randomwalk_iter():
    print num,
In fact, even the idiom if elem in iterator is supported, which lazily only tries as many elements of the iterator as are needed to determine the truth value (if it winds up false, it needs to try all the elements, of course).

Leaving a trail of crumbs
The above approaches are fine for the problem at hand. But none of them scale very well to the case where a routine creates a large number of local variables along the way, and winds its way into a nest of loops and conditionals. If an iterator class or a function with static (or global) variables depends on multiple data states, two problems come up. One is the mundane matter of creating multiple instance attributes or static list elements to hold each of the data values. The far more important problem is figuring out how to get back to exactly the relevant part of the flow logic that corresponds to the data states. It is awfully easy to forget about the interaction and codependence of different data.
Generators simply bypass the whole problem. A generator "returns" with the new keyword yield, but "remembers" the exact point of execution where it returned. Next time the generator is called, it picks up where it left before -- both in terms of function flow and in terms of variable values.
One does not directly write a generator in Python 2.2+. Instead, one writes a function that, when called, returns a generator. This might seem odd, but "function factories" are a familiar feature of Python, and "generator factories" are an obvious conceptual extension of this. What makes a function a generator factory in Python 2.2+ is the presence of one or more yield statements somewhere in its body. If yield occurs, return must only occur without any accompanying return value. A better choice, however, is to arrange the function bodies so that execution just "falls off the end" after all the yields are accomplished. But if a return is encountered, it causes the produced generator to raise a StopIteration exception rather than yield further values.
In my opinion, the choice of syntax for generator factories was somewhat poor. A yield statement can occur well into the body of a function, and you might be unable to determine that a function is destined to act as a generator factory anywhere within the first N lines of a function. The same thing could, of course, be true of a function factory -- but being a function factory doesn't change the actual syntax of a function body (and a function body is allowed to sometimes return a plain value; albeit probably not out of good design). To my mind, a new keyword -- such as generator in place of def -- would have been a better choice.
Quibbles over syntax aside, generators have the good manners to automatically act as iterators when called on to do so. Nothing like the .__iter__() method of classes is needed here. Every yield encountered becomes a return value for generator's .next() method. Let's look at the simplest generator to make things clear:
>>> from __future__ import generators
>>> def gen():
        yield 1

>>> g = gen()
>>> g.next()
1
>>> g.next()
Traceback (most recent call last):
  File "<pyshell#15>", line 1, in ?
    g.next()
StopIteration
Let's put a generator to work in our sample problem:
from __future__ import generators   # only needed for Python 2.2
import random
def  randomwalk_generator():
    last, rand = 1, random.random() # initialize candidate elements
    while rand > 0.1:                # threshhold terminator
        print '*',                   # display the rejection
        if abs(last-rand) >= 0.4:    # accept the number
            last = rand             # update prior value
            yield rand              # return AT THIS POINT
        rand = random.random()      # new candidate
    yield rand                      # return the final small element
The simplicity of this definition is appealing. You can utilize the generator either manually or as an iterator. In the manual case, the generator can be passed around a program, and called wherever and whenever needed (which is quite flexible). A simple example of the manual case is:
gen = randomwalk_generator()
try:
    while 1: print gen.next(),
except StopIteration:
    pass
Most frequently, however, you are likely to use a generator as an iterator, which is even more concise (and again looks just like an old-fashioned sequence):
for num in randomwalk_generator():
    print_short(num)

Yielding
It will take a little while for Python programmers to become familiar with the ins-and-outs of generators. The added power of such a simple construct is surprising at first; and even quite accomplished programmers (like the Python developers themselves) will continue to discover subtle new techniques using generators for some time, I predict.
To close, let me present one more generator example that comes from the test_generators.py module distributed with Python 2.2. Suppose you have a tree object, and want to search its leaves in left-to-right order. Using state-monitoring variables, getting a class or function just right is difficult. Using generators makes it almost laughably easy:
>>>> # A recursive generator that generates Tree leaves in in-order.
>>> def inorder(t):
...     if t:
...         for x in inorder(t.left):
...             yield x
...         yield t.label
...         for x in inorder(t.right):
...             yield x


What exactly are Python's iterator, iterable, and iteration protocols?

Iteration is a general term for taking each item of something, one after another. Any time you use a loop, explicit or implicit, to go over a group of items, that is iteration.

In Python, iterable and iterator have specific meanings.

An iterable is an object that has an __iter__ method which returns an iterator, or which defines a __getitem__ method that can take sequential indexes starting from zero (and raises an IndexError when the indexes are no longer valid). So an iterable is an object that you can get an iterator from.

An iterator is an object with a next (Python 2) or __next__ (Python 3) method.

Whenever you use a for loop, or map, or a list comprehension, etc. in Python, the next method is called automatically to get each item from the iterator, thus going through the process of iteration.

A good place to start learning would be the iterators section of the tutorial [https://docs.python.org/3/tutorial/classes.html#iterators] and the iterator types section of the standard types page [https://docs.python.org/dev/library/stdtypes.html#iterator-types]. After you understand the basics, try the iterators section of the Functional Programming HOWTO [https://docs.python.org/dev/howto/functional.html#iterators].

Iterators [https://docs.python.org/3/tutorial/classes.html#iterators]

By now you have probably noticed that most container objects can be looped over using a for statement:

for element in [1, 2, 3]:
    print(element)
for element in (1, 2, 3):
    print(element)
for key in {'one':1, 'two':2}:
    print(key)
for char in "123":
    print(char)
for line in open("myfile.txt"):
    print(line, end='')
This style of access is clear, concise, and convenient. The use of iterators pervades and unifies Python. Behind the scenes, the for statement calls iter() on the container object. The function returns an iterator object that defines the method __next__() which accesses elements in the container one at a time. When there are no more elements, __next__() raises a StopIteration exception which tells the for loop to terminate. You can call the __next__() method using the next() built-in function; this example shows how it all works:

>>> s = 'abc'
>>> it = iter(s)
>>> it
<iterator object at 0x00A1DB50>
>>> next(it)
'a'
>>> next(it)
'b'
>>> next(it)
'c'
>>> next(it)
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
    next(it)
StopIteration
Having seen the mechanics behind the iterator protocol, it is easy to add iterator behavior to your classes. Define an __iter__() method which returns an object with a __next__() method. If the class defines __next__(), then __iter__() can just return self:

class Reverse:
    """Iterator for looping over a sequence backwards."""
    def __init__(self, data):
        self.data = data
        self.index = len(data)
    def __iter__(self):
        return self
    def __next__(self):
        if self.index == 0:
            raise StopIteration
        self.index = self.index - 1
        return self.data[self.index]
>>> rev = Reverse('spam')
>>> iter(rev)
<__main__.Reverse object at 0x00A1DB50>
>>> for char in rev:
...     print(char)
...
m
a
p
s

4.5. Iterator Types [https://docs.python.org/dev/library/stdtypes.html#iterator-types]

Python supports a concept of iteration over containers. This is implemented using two distinct methods; these are used to allow user-defined classes to support iteration. Sequences, described below in more detail, always support the iteration methods.

One method needs to be defined for container objects to provide iteration support:

container.__iter__()
Return an iterator object. The object is required to support the iterator protocol described below. If a container supports different types of iteration, additional methods can be provided to specifically request iterators for those iteration types. (An example of an object supporting multiple forms of iteration would be a tree structure which supports both breadth-first and depth-first traversal.) This method corresponds to the tp_iter slot of the type structure for Python objects in the Python/C API.

The iterator objects themselves are required to support the following two methods, which together form the iterator protocol:

iterator.__iter__()
Return the iterator object itself. This is required to allow both containers and iterators to be used with the for and in statements. This method corresponds to the tp_iter slot of the type structure for Python objects in the Python/C API.

iterator.__next__()
Return the next item from the container. If there are no further items, raise the StopIteration exception. This method corresponds to the tp_iternext slot of the type structure for Python objects in the Python/C API.

Python defines several iterator objects to support iteration over general and specific sequence types, dictionaries, and other more specialized forms. The specific types are not important beyond their implementation of the iterator protocol.

Once an iterator’s __next__() method raises StopIteration, it must continue to do so on subsequent calls. Implementations that do not obey this property are deemed broken.

Iterators [https://docs.python.org/dev/howto/functional.html#iterators]

I’ll start by looking at a Python language feature that’s an important foundation for writing functional-style programs: iterators.

An iterator is an object representing a stream of data; this object returns the data one element at a time. A Python iterator must support a method called __next__() that takes no arguments and always returns the next element of the stream. If there are no more elements in the stream, __next__() must raise the StopIteration exception. Iterators don’t have to be finite, though; it’s perfectly reasonable to write an iterator that produces an infinite stream of data.

The built-in iter() function takes an arbitrary object and tries to return an iterator that will return the object’s contents or elements, raising TypeError if the object doesn’t support iteration. Several of Python’s built-in data types support iteration, the most common being lists and dictionaries. An object is called iterable if you can get an iterator for it.

You can experiment with the iteration interface manually:

>>> L = [1,2,3]
>>> it = iter(L)
>>> it  
<...iterator object at ...>
>>> it.__next__()  # same as next(it)
1
>>> next(it)
2
>>> next(it)
3
>>> next(it)
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
StopIteration
>>>
Python expects iterable objects in several different contexts, the most important being the for statement. In the statement for X in Y, Y must be an iterator or some object for which iter() can create an iterator. These two statements are equivalent:

for i in iter(obj):
    print(i)

for i in obj:
    print(i)
Iterators can be materialized as lists or tuples by using the list() or tuple() constructor functions:

>>> L = [1,2,3]
>>> iterator = iter(L)
>>> t = tuple(iterator)
>>> t
(1, 2, 3)
Sequence unpacking also supports iterators: if you know an iterator will return N elements, you can unpack them into an N-tuple:

>>> L = [1,2,3]
>>> iterator = iter(L)
>>> a,b,c = iterator
>>> a,b,c
(1, 2, 3)
Built-in functions such as max() and min() can take a single iterator argument and will return the largest or smallest element. The "in" and "not in" operators also support iterators: X in iterator is true if X is found in the stream returned by the iterator. You’ll run into obvious problems if the iterator is infinite; max(), min() will never return, and if the element X never appears in the stream, the "in" and "not in" operators won’t return either.

Note that you can only go forward in an iterator; there’s no way to get the previous element, reset the iterator, or make a copy of it. Iterator objects can optionally provide these additional capabilities, but the iterator protocol only specifies the __next__() method. Functions may therefore consume all of the iterator’s output, and if you need to do something different with the same stream, you’ll have to create a new iterator.

Data Types That Support Iterators
We’ve already seen how lists and tuples support iterators. In fact, any Python sequence type, such as strings, will automatically support creation of an iterator.

Calling iter() on a dictionary returns an iterator that will loop over the dictionary’s keys:

>>> m = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
...      'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}
>>> for key in m:  
...     print(key, m[key])
Mar 3
Feb 2
Aug 8
Sep 9
Apr 4
Jun 6
Jul 7
Jan 1
May 5
Nov 11
Dec 12
Oct 10
Note that the order is essentially random, because it’s based on the hash ordering of the objects in the dictionary.

Applying iter() to a dictionary always loops over the keys, but dictionaries have methods that return other iterators. If you want to iterate over values or key/value pairs, you can explicitly call the values() or items() methods to get an appropriate iterator.

The dict() constructor can accept an iterator that returns a finite stream of (key, value) tuples:

>>> L = [('Italy', 'Rome'), ('France', 'Paris'), ('US', 'Washington DC')]
>>> dict(iter(L))  
{'Italy': 'Rome', 'US': 'Washington DC', 'France': 'Paris'}
Files also support iteration by calling the readline() method until there are no more lines in the file. This means you can read each line of a file like this:

for line in file:
    # do something for each line
    ...
Sets can take their contents from an iterable and let you iterate over the set’s elements:

S = {2, 3, 5, 7, 11, 13}
for i in S:
    print(i)
Generator expressions and list comprehensions
Two common operations on an iterator’s output are 1) performing some operation for every element, 2) selecting a subset of elements that meet some condition. For example, given a list of strings, you might want to strip off trailing whitespace from each line or extract all the strings containing a given substring.

List comprehensions and generator expressions (short form: “listcomps” and “genexps”) are a concise notation for such operations, borrowed from the functional programming language Haskell (http://www.haskell.org/). You can strip all the whitespace from a stream of strings with the following code:

line_list = ['  line 1\n', 'line 2  \n', ...]

# Generator expression -- returns iterator
stripped_iter = (line.strip() for line in line_list)

# List comprehension -- returns list
stripped_list = [line.strip() for line in line_list]
You can select only certain elements by adding an "if" condition:

stripped_list = [line.strip() for line in line_list
                 if line != ""]
With a list comprehension, you get back a Python list; stripped_list is a list containing the resulting lines, not an iterator. Generator expressions return an iterator that computes the values as necessary, not needing to materialize all the values at once. This means that list comprehensions aren’t useful if you’re working with iterators that return an infinite stream or a very large amount of data. Generator expressions are preferable in these situations.

Generator expressions are surrounded by parentheses (“()”) and list comprehensions are surrounded by square brackets (“[]”). Generator expressions have the form:

( expression for expr in sequence1
             if condition1
             for expr2 in sequence2
             if condition2
             for expr3 in sequence3 ...
             if condition3
             for exprN in sequenceN
             if conditionN )
Again, for a list comprehension only the outside brackets are different (square brackets instead of parentheses).

The elements of the generated output will be the successive values of expression. The if clauses are all optional; if present, expression is only evaluated and added to the result when condition is true.

Generator expressions always have to be written inside parentheses, but the parentheses signalling a function call also count. If you want to create an iterator that will be immediately passed to a function you can write:

obj_total = sum(obj.count for obj in list_all_objects())
The for...in clauses contain the sequences to be iterated over. The sequences do not have to be the same length, because they are iterated over from left to right, not in parallel. For each element in sequence1, sequence2 is looped over from the beginning. sequence3 is then looped over for each resulting pair of elements from sequence1 and sequence2.

To put it another way, a list comprehension or generator expression is equivalent to the following Python code:

for expr1 in sequence1:
    if not (condition1):
        continue   # Skip this element
    for expr2 in sequence2:
        if not (condition2):
            continue    # Skip this element
        ...
        for exprN in sequenceN:
             if not (conditionN):
                 continue   # Skip this element

             # Output the value of
             # the expression.
This means that when there are multiple for...in clauses but no if clauses, the length of the resulting output will be equal to the product of the lengths of all the sequences. If you have two lists of length 3, the output list is 9 elements long:

>>> seq1 = 'abc'
>>> seq2 = (1,2,3)
>>> [(x, y) for x in seq1 for y in seq2]  
[('a', 1), ('a', 2), ('a', 3),
 ('b', 1), ('b', 2), ('b', 3),
 ('c', 1), ('c', 2), ('c', 3)]
To avoid introducing an ambiguity into Python’s grammar, if expression is creating a tuple, it must be surrounded with parentheses. The first list comprehension below is a syntax error, while the second one is correct:

# Syntax error
[x, y for x in seq1 for y in seq2]
# Correct
[(x, y) for x in seq1 for y in seq2]


Understanding __get__ and __set__ and Python descriptors

The descriptor is how python's property type is implemented. A descriptor simply implements __get__, __set__, etc. and is then added to another class in its definition (as you did above with the Temperature class). For example

temp=Temperature()
temp.celsius #calls Celsius.__get__
Accessing the property you assigned the descriptor to (celsius in the above example) calls the appropriate descriptor method.

instance in __get__ is the instance of the class (so above, __get__ would receive temp, while owner is the class with the descriptor (so it would be Temperature).

You need to use a descriptor class to encapsulate the logic that powers it. That way, if the descriptor is used to cache some expensive operation (for example), it could store the value on itself and not its class.

An article about descriptors can be found at http://martyalchin.com/2007/nov/23/python-descriptors-part-1-of-2/

EDIT: As jchl pointed out in the comments, if you simply try Temperature.celsius, instance will be None.

Python Descriptors, Part 1 of 2 [http://martyalchin.com/2007/nov/23/python-descriptors-part-1-of-2/]
by Marty Alchin on November 23, 2007 about Django
I rather enjoyed writing about a relatively underused feature yesterday, so today is more of the same. Of course, continuing with a focus on Django, today’s Python feature is also commonly used throughout a number of Django’s internals: descriptors. Python’s documentation on descriptors is rather sparse, though there’s a great writeup on it already. I won’t try to reinvent the wheel in its entirety here, I’ll just write up some basic details and how it can be used for Django.
What are descriptors?
In a nutshell, a descriptor is a way to customize what happens when you reference an attribute on a model. Normally, Python just gets and sets values on attributes without any special processing. It’s just basic storage. Sometimes, however, you might want to do more. You might need to validate the value that’s being assigned to a value. You may want to retrieve a value and cache it for later use, so that future references don’t have all the overhead.
These are all things that would normally need to be done with a method, but if you’ve already started with a basic attribute, changing to a method would require changing all the code that uses the attribute to use a method call instead. This potential change is a primary motivation for typical Java programs to always use methods even for basic attribute access. The common pattern is to have all attributes private, and provide public access through methods, simply to accommodate potential future changes in the internals of that attribute access.
Python’s descriptors as an alternative approach. Instead of starting with methods all the time, you can start with basic attributes and write all the code you want. Then, if you ever need advanced processing to occur when you access those attributes, you can just add in a descriptor to do the work, without updating all the other code. In my opinion, this makes the referring code much cleaner, without having to deal with method calls for attributes. But that’s an issue outside the scope of this article.
But the real detail of a descriptor is that it’s a Python object that’s assigned as an attribute of a class. The object is an instance of a class that’s defined in a particular way (described briefly below), and the attribute it’s assigned to is the one that will have the special processing. So the actual extra code will be inside the descriptor’s class, rather than the class it will be assigned to. This may seem a little weird, but it also makes some sense.
Some examples of descriptors in Django:
Model managers
ForeignKey fields (usage)
…others that I’m not thinking of right now (sorry, it’s late!)
Descriptor protocol
A descriptor is implemented as a standard new-style class in Python, and it doesn’t need to inherit from anything in particular besides object. The real trick to building a descriptor is defining at least one of the following three methods. Note that instance below returns to the object where the attribute was accessed, and owner is the class where the descriptor was assigned as an attribute.
__get__(self, instance, owner) — This will be called when the attribute is retrieved (value = obj.attr), and whatever it returns is what will be given to the code that requested the attribute’s value.
__set__(self, instance, value) — This gets called when a value is set to the attribute (obj.attr = 'value'), and shouldn’t return anything at all.
__delete__(self, instance) — This is called when the attribute is deleted from an object (del obj.attr)
Astute readers will quickly notice — and perhaps be confused by — the fact that only __get__ can receive the owner class, while the rest only receive the instance. Descriptors are assigned to a class, not to an instance, and modifying the class would actually overwrite or delete the descriptor itself, rather than triggering its code. This intentional. Otherwise, once a descriptor is set, it could never be removed or modified without modifying source code and restarting the program. That’s not preferable, so only the retrieval method has access to the owner class. It will always be set to the appropriate class, though instance may be None if the attribute was accessed from the class. This is what Django uses to throw an error if you try to access a manager from an object instead of a class, or a related model (ForeignKey) on a class instead of an object.
More than just the protocol
Also, since descriptors are standard classes that just implement a specific set of methods, they can also contain anything else used on standard Python classes. This is especially useful when defining __init__ on a descriptor class, so that you can customize descriptors for individual attributes. For example, the following descriptor simulates rolling a die:
import random

class Die(object):
    def __init__(self, sides=6):
        self.sides = sides

    def __get__(self, instance, owner):
        return int(random.random() * self.sides) + 1
This will default to a 6-sided die, but the number of sides can be explicitly defined on a per-attribute basis. Then, when the attribute is accessed, the __get__ method will be called, returning a random number based on the number of sides the die was created with.
Using descriptors
As mentioned above, descriptors are assigned to classes, and the special methods are called automatically when the attribute is accessed, and the method used depends on what type of access is being performed. The Die example above might be used as follows:
class Game(object):
    d6 = Die()
    d10 = Die(sides=10)
    d20 = Die(sides=20)
Then, these special attributes can be accessed either on that class or on any instances of it.
>>> Game.d6
5
>>> Game.d10
8
>>> Game.d20
19
>>> Game.d20
3
>>> game = Game()
>>> game.d20
12

Python Descriptors, Part 2 of 2 [http://martyalchin.com/2007/nov/24/python-descriptors-part-2-of-2/]
by Marty Alchin on November 24, 2007 about Django
Yesterday, I gave a basic overview of descriptors and what they can do, including a simple example to demonstrate one in action. That’s all well and good, but today I’ll explain how this can be genuinely useful in your apps, particularly when used in models.
Storing data with a descriptor
Yesterday’s example generated a new value each time it was accessed, which is really only useful in a few situations. More often, you’ll need to still store a value somewhere, and just do something special when its modified or retrieved. There are a few ways to approach this, but I’ll just cover one.
The simplest way to store a value for a desciptor takes advantage of a subtle distinction of how Python accesses values on an instance object. Every Python object has a namespace that’s separate from the namespace of its class, so that each object can have different values attached to it. Normally, the object’s attributes are a direct pass-through to this namespace, but descriptors short-circuit that process. Thankfully, Python allows another way to access the object’s namespace directly: the __dict__ attribute of the object.
Every object has a __dict__ attribute, which is a standard Python dictionary containing mappings for the various values attached to it. Even though descriptors get in the way of how this is normally accessed, your code can use __dict__ to get at it directly, and it’s a great place to store a single value. Yesterday, I mentioned that descriptors can be used to cache values to speed up subsequent accesses, and this is a good way to go about that.
from myapp.utils import retrieve

class CachedValue(object):
    def __init__(self, name):
        self.name = name

    def __get__(self, instance, owner):
        if self.name not in instance.__dict__:
            instance.__dict__[self.name] = retrieve()
        return instance.__dict__[self.name]
Of course, you’ll notice something interesting here. We have to assign the value to the dictionary using a name, and the only way we know what name to use is to supply it explicitly. For this example, the constructor takes a required name argument, which will be used for the dictionary’s key, but Django provides a much better way to solve this problem. More on that later.
So far, the examples have only involved retrieval. This technique is easily extended to allow the value to be modified as well, by adding a __set__ method. The following example should look fairly straightforward, now that you know the __dict__ technique:
class SimpleDescriptor(object):
   def __init__(self, name):
        self.name = name

    def __get__(self, instance, owner):
        if self.name not in instance.__dict__:
            raise AttributeError, self.name
        return instance.__dict__[self.name]

    def __set__(self, instance, value):
        instance.__dict__[self.name] = value
This also illustrates another interesting point with __get__: if the value being retrieved isn’t set, the expected behavior is to raise an AttributeError. That’s what Python does internally, and that’s what most code will be expecting when this occurs.
Subclassing Field
One of the most important uses for descriptors in Django is when creating new Field types for use with models. There’s a lot that can be done when creating new field types, but that’s a topic for its own series of posts, perhaps some other time. For today’s purposes, I’ll just cover how descriptors can help the process along. Descriptors are especially useful for model fields, because they allow you to integrate specialized Python types with the standard Django database api.
Of course, his is all fairly educational. This particular process is done much more easily by Django now, if you’re tracking trunk. This information is still good to know, though, because the official support in Django uses descriptors behind the scenes, and not all situations are covered, so you might need to implement this yourself if you find the need.
The base class, Field, makes use of a special hook in Django, by defining a method called contribute_to_class, and many subclasses override this to provide their own functionality. Again, I won’t get into everything that’s possible with this, but it provides a very simple solution to our naming problem. Essentially, this method gets called for any object that defines it, instead of being simply attached to the class as normal. The method uses the following definition:
def contribute_to_class(self, cls, name)
self - the object being assigned to the model
cls - the model class the object is being assigned to
name - the name that was used in the assignment.
That’s right, contribute_to_class gets the name that was given to the object when it was assigned, so we don’t have to expect anyone to provide it explicitly!
Further reading
This isn’t a complete tutorial for subclassing Field, just like the last one wasn’t a complete discussion of descriptors. There’s plently more that can be done, but the best place I can point to is the source for GeoDjango, where Robert Coup so brilliantly implemented descriptors for a very specialized use case. Beyond that, be sure to read the source to Malcolm’s recent addition to Django’s source, to make this all a lot easier.